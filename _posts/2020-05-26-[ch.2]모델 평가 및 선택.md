---
title: "[예제로 공부하는 단단한 머신러닝] chapter 02. 모델 평가 및 선택"
date: 2020-05-26 00:00:00 -0400
categories: MachineLearning
---

# 2.1 경험 오차 및 과적합
- 오차율(error rate, E) : 잘못 분류한 샘플 수(a) / 전체 샘플 수(m)
- 정밀도(accuracy) : 1 - a/m = 1 - E
- 오차(error) : 실제 예측값과 샘플의 실제 값 사이의 차이
- 훈련 오차(training error) 혹은 경험 오차(empirical error) : 학습기가 훈련 세트상에서 만들어낸 오차
- 일반화 오차(generalization error) : 학습기가 새로운 샘플에서 만들어낸 오차

(대부분) 머신러닝의 목적은 일반화 오차가 가장 적은 학습기(모델)을 만들어 내는 것이다. 하지만, 우리는 새로운 데이터(실제 데이터)에 대해서 알 수 없고, 할 수 있는 것은 훈련 세트를 이용해서 경험 오차를 최대한 줄이는 것이다.

새로운 데이터에서 훌륭한 성능을 내는 모델을 만들기 위해, 우리는 훈련 데이터에서 잠재적인 '보편 규칙'을 찾아내어 모델을 학습시켜야 한다.

- 과적합(overfitting) : 훈련데이터'만'의 특징을 학습하여 훈련 오차는 낮지만, 일반화 성능이 떨어지는 상태
- 과소적합(underfitting) : 훈련데이터의 특징도 제대로 학습하지 못해 훈련 오차가 큰 상태

과적합이 일어나는 이유는 다양하다. (책에서는) 학습능력이 너무 뛰어나 훈련 데이터로부터 일반적이지 않은 특징까지 학습하는 경우가 가장 흔하다고 한다. 그 외에 과적합이 일어나는 경우는, 훈련 데이터가 실제 데이터와 다를 때(훈련 데이터가 분포를 충분히 포함하지 않을 때?) 혹은 훈련을 너무 오래 시켰을 때 등이 있을 것 같다.

과소적합의 경우, 과적합과 반대로 훈련을 충분히 시키지 않았거나 모델의 학습능력이 부족해서 발생하곤 한다.

대부분의 머신러닝 문제의 경우, 과소적합보다는 과적합을 방지하기 위해 여러가지 조치를 취해서 과적합의 정도를 최소화하려고 노력한다.

학습 알고리즘을 고르고, 일정한 파라미터를 선정하여 학습을 진행하여 모델을 얻고나면, 얻어진 모델들 중 어떤 것을 사용할 것 인가? 에 대한 고민을 하게된다. 이러한 과정을 **모델 선택**(model selection) 문제라고 한다. 일반화 오차를 측정할 수 있다면, 이를 이용해서 모델을 선택하면 될텐데, 일반적인 상황에서 일반화 오차는 측정할 수 없으므로 모델을 평가하기 위한 방법을 모색해야 한다.

# 2.2 평가 방법
평가는 테스트 세트(testing set)를 사용해서 진행된다. 테스트 세트는 훈련에 사용되지 않은(않도록 한) 데이터들로, 실제 샘플과 동일한 분포로 가정하여 테스트 세트를 이용해 측정한 테스트 오차(testing error)를 측정한다. 측정된 테스트 오차는 일반화 오차의 근사치로 추정한다.

즉, 의미있는 평가를 진행하기 위해서는 전체 데이터 세트에서 훈련 세트와 테스트 세트를 잘 나누는 것이 중요하다.

## 2.2.1 홀드아웃
홀드아웃(hold-out) 방법(검증 세트 기법)은 데이터 세트를 겹치지 않는 두 집단으로 나눠서, 하나를 훈련 세트(S) 다른 하나를 테스트 세트(T)로 사용하는 방법이다.

주의해야 할 점은 훈련 세트와 테스트 세트를 나눌 때, 되도록 데이터 분포가 같게 나눠야 한다. 잘못된 구분은 각 세트에 대한 편향을 가지게 할 수 있기 때문이다. 또한, 훈련/테스트 비율을 선정하는 것 역시 중요하다.

임의로 나누어진 훈련/테스트 세트는 나눠서 학습을 진행할 때마다 모델의 학습과 평가 결과가 다를 수 있기 때문에(훈련/테스트 데이터 분포가 매번 달라질 수 있기 때문에), 여러번 수행하여 학습 결과를 검증하는 것이 중요하다.

## 2.2.2 교차 검증
교차 검증(cross validation)은 데이터 세트를 중복하지 않는 k개의 집합으로 나눈 뒤, k-1개를 학습 세트로 사용하고 나머지 하나를 테스트 세트로 사용하는 방법이다. 이렇게 하면, k번의 훈련과 테스트가 가능하고, 결과값의 평균을 이용하여 검증을 할 수 있다. 이러한 방식은 k의 값에 따라 안정성과 정확도가 달라지는데, 이를 k겹 교차 검증(k-fold cross validation)이라고 부른다. 보통은 k값에 5, 10 등의 숫자를 자주 사용한다.

k 값이 커지면, 훈련과 테스트에 사용되는 연산량이 많이 필요하기 때문에 적절한(?) 숫자를 선택하는 것이 필요하다.

## 2.2.3 부트스트래핑
부트스트래핑(bootstrapping)은 부트스트랩 샘플링(bootstrap sampling)에 기반을 둔 샘플 추출 기법으로, 중복 추출을 허용하여 전체 데이터 세트에서 m만큼 데이터를 뽑아내서 데이터 세트를 만드는 방식이다. 이 방식으로 추출된 데이터를 학습에 사용하고, 나머지 데이터를 테스트에 사용하는 방식을 Out-of-Bag 예측이라고 한다.

부트스트래핑은 데이터 세트가 비교적 적거나, 훈련/테스트 세트로 분류하기 힘들때 사용하기 좋다. 또, 데이터 세트에서 다양한 (분포를 가지는) 훈련 세트를 만들 수 있기 때문에, 앙상블 기법에 적용하기 좋다.

하지만, 데이터가 충분하다면 일반적으로 홀드 아웃이나 교차 검증 방법을 사용한다.

## 2.2.4 파라미터 튜닝과 최종 모델
대부분 학습 알고리즘은 파라미터를 선택해야 한다. 모델 평가 및 선택을 할 때, 학습 알고리즘 자체뿐 만 아니라 알고리즘 파라미터도 모델의 성능에 영향을 미치 때문에, 파라미터를 선택(혹은 조율)하는 파라미터 튜닝(parameter tuning)이 필요하다.

알고리즘 파라미터 중 많은 부분이 실수 범위의 값을 가지기 때문에, 최적의 파라미터를 찾는 것은 매우 어려운 일이다. 결국 절충안으로 몇 가지 값을 취사선택해서 테스트해보는 수 밖에 없다.

파라미터가 적절한지 테스트해보기 위해서 사용하는 테스트 데이터 집합을 검증 세트(validation set)이라고 부른다. 테스트 세트와 잘 구분이 되어야 하는데, 테스트 세트는 학습기의 최종 성능을 측정을 위해 사용되고, 검증 세트는 모델의 학습 정도 판단과 파라미터 튜닝 등을 통해 학습단계에서 더 나은 모델을 얻기 위해 사용되는 것이다.

# 2.3 모델 성능 측정
모델의 일반화 성능을 평가하기 위해서는 이를 위한 기준이 필요하다. 이러한 기준을 성능 측정(perfermance measure)라고 한다. 성능 측정의 목적은 프로젝트에 따라 다를 수 있기 때문에, 데이터 분석 목적에 따라 적절한 성능 측정 방식을 채택해야 한다.

회귀분석에서는 대개 평균제곱오차(Mean Squared Error, MSE)를 사용한다.

## 2.3.1 오차율과 정확도
가장 자주 사용되는 성능 척도는 오차율(Error Rate)과 정확도(Accuarcy)이다.

오차율을 전체 샘플 중에서 잘못 분류된 샘플들의 비율이고, 정확도는 전체 샘플 중에서 제대로 분류된 샘플들의 비율이다. 결국 오차율과 정확도의 합은 1로 표현된다.

## 2.3.2 재현율, 정밀도 그리고 F1 스코어
오차율과 정확도만으로 성능 측정이 충분하지 않을 수 있다. 정밀도(Precision)과 재현율(Recall)를 이용하면, 오차율과 정확도가 표현하지 못하는 것을 측정할 수 있다.

정밀도는 전체 양(Positive)의 예측 중에 실제 양의 값을 가지는 샘플의 비율이고, 재현율은 실제 양의 값을 가지는 총 샘플 중에 양의 값으로 예측된 비율이다.

정밀도와 재현율은 트레이드오프(trade-off) 관계이다. 그렇기 때문에, 일반적으로 정밀도가 높으면 재현율이 낮고, 재현율이 높으면 정밀도가 낮다(매우 간단한 문제에서만 비교적 높은 정밀도와 재현율을 달성할 수 있다.)

정밀도와 재현율의 트레이드오프 관계는 임계값(threshold)로 확인할 수 있다. 어떤 샘플에 대해서 학습기를 통해서 분류 결과는 임의의 확률로 표현될 수 있다. 이 확률값을 활용하여 참/거짓(혹은 양성/음성)을 판단할 때, 어느 정도의 임계값을 선택하냐에 따라 정밀도와 재현율이 달라질 수 있다(임계값을 높이면, 확률이 높은 것만 양으로 선택하게 되므로, 정밀도는 올라갈 수 있지만 재현율은 내려갈 것이다.)

학습기의 정밀도와 재현율을 그래프로 그리면 P-R곡선을 나타낼 수 있다. 이는 샘플 전체에 대한 임의의 학습기의 정밀도, 재현율을 나타낸다. 하지만 정밀도와 재현율, 그리고 P-R곡선만으로 학습기들간의 성능을 비교하기는 쉽지않다.

P-R곡선의 면적을 계산하여 이를 척도로 사용할 수도 있으나, 이는 계산하기가 까다롭기 때문에, 손익분기점(Break-Even Point, BEP)를 사용하여 성능을 측정하기도 한다. BEP는 정밀도와 재현율이 같을 때의 값을 나타낸다.

하지만 BEP보다 F1 스코어를 더 많이 사용한다.
- F(1) = ( 2 * Precision * Recall ) / ( Precision + Recall )

혹은, 가중치를 사용하는
- F(b) = ( 1+b^2 ) * Precision * Recall / (( b^2 * Precision ) + Recall)
을 사용한다.

## 2.3.3 ROC와 AUC
대부분의 학습기는 데스트 세트에 대한 결과로 실수 값 혹은 확률을 출력한다. 출력된 결과 값을 사용하기 위해서는 임계치(threshold)가 필요한데, 확률의 경우 임계치를 0.5 등의 숫자를 사용한다.

임계치를 또다른 단어로 차단점(cut point)로 표현할 수 있다. 이 차단점의 위치를 조정함에 따라 정밀도 혹은 재현율에 가중치를 둔 결과를 얻을 수 있다.

이러한 결과를 위해 ROC(Receiver Operating Characteristic, 수신기 조작 특성) 곡선을 사용할 수 있다. ROC 곡선은 P-R곡선과 유사하게, 모델의 예측 결과를 기반으로 (테스트) 샘플의 순서를 매기고, 해당 순서에 따라 양성값이 될 확률을 계산한다. 그리고, TPR(True Positive Rate, 참 양성률)과 FPR(False Positive Rante, 거짓 양성율)값을 계산하여 x축과 y축에 그려넣은 것이 ROC 곡선이다.

- TPF(참 양성률)은 실제로 양성인 것 중에 양성으로 예측된 것이고,
- FPR(거짓 양성률)은 실제로 음성인 것 중에 양성으로 예측된 것이다.

TPF는 Recall(재현율)과 같다. TPF와 FPR은 ROC 곡선 상에서 어느정도 비례하는 관계를 보이는데, 실제로 양성인 것을 양성으로 예측하기 위해서, 더 많은 샘플들을 양성으로 예측하기 위해 차단점이 점점 내려가야하고, 이러다보면 실제로 음성인데 양성으로 예측하게 되는 샘플도 점점 많아질 수 밖에 없다.

ROC 그래프에서 (0,1)은 모든 양성값을 올바르게 양성으로 분류한 '가장 이상적인 모델'로 볼 수 있다.

실제 샘플 수는 한정적이기 때문에 부드러운 곡선이 나오는 것이 아니라 계단 형식의 그래프를 그리게 될 것 이다. 이 때, 그래프 아래의 면적(AUC, Area Under ROC curve)를 사용하여 서로 다른 두 개의 모델(학습기)이 성능을 비교해볼 수 있다(절대적인 우위를 나타내지는 않는다.)

## 2.3.4 비용민감 오차율과 비용 곡선

잘 못 예측힌 결과에 대한 가중치를 줘서, 비용을 구하는 방식으로 예측이 빗나갓을 때 결과의 영향도가 다를 수 있기 때문에 이와 같은 방식을 사용하기도 한다.

# 2.4 비교 검증
