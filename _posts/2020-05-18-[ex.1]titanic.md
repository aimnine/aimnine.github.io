---
title: "[예제로 공부하는 단단한 머신러닝] example 01. 타이타닉 생존자 예측"
date: 2020-05-18 00:00:00 -0400
categories: MachineLearning
---

첫 예제는 타이타닉 생존자 예측 문제이다.

워낙 유명한 문제이기도 하고, 케글(kaggle) 등에 다양한 해법들이 이미 존재하고 있다.

튜토리얼도 많이 존재하기 때문에, 첫 번째 예제로 적합하다고 생각했고, 실제로 tensorflow의 공식 튜토리얼을 참고했다.

- Tensorflow tutorial : https://www.tensorflow.org/tutorials/load_data/csv?hl=ko

예제를 진행하기 위해서는 파이썬 개발환경(Tensorflow 등 기타 파이썬 패키지가 포함된)이 필요한데, 가장 접근이 용이한 google colab을 사용하는 것을 추천한다.

# 1. Setup
- 머신러닝을 수행하기 위해서, 필요한 파이썬 라이브러리를 불러오고,
<pre>
<code class="python">
import functools

import numpy as np
import tensorflow as tf
</code>
</pre>
- 데이터를 가져오는 과정이다.
<pre>
<code class="python">
TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"

train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
</code>
</pre>
- 예제(혹은 튜토리얼) 문제들은 비교적 예쁘게? 데이터가 정리되어있지만, 실제 머신러닝이 적용되어야 할 문제들에서는 이와 같은 작업이 선행되어 있지않다.
- 머신러닝에 관련된 발표를 듣다보면, 데이터를 모으고, 처리하는 과정(예제의 경우, .csv파일이 그 결과)이 전체 머신러닝 프로세스의 70%(혹은 그 이상)이라고 한다.
- 또, 많이 이야기 하는 것이 **Garbage In, Garbage Out**으로 머신러닝으로 문제를 해결하기 위해 좋은? 데이터를 사용하는 것도 중요하다고 한다(Garbage가 아닌 데이터가 무엇일지 생각해보자)

# 2. Load data
- 앞선 과정에서, 문제에 적용하기 위해서 데이터를 가져왔다.
- 가져온 데이터 중 학습 데이터를 간략하게 살펴본다(쉘스크립트)
<pre>
<code class="shell">
head {train_file_path}
</code>
</pre>
- 대략 이런 데이터들을 볼 수 있다.
<pre>
<code>
survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone
0,male,22.0,1,0,7.25,Third,unknown,Southampton,n
1,female,38.0,1,0,71.2833,First,C,Cherbourg,n
1,female,26.0,0,0,7.925,Third,unknown,Southampton,y
1,female,35.0,1,0,53.1,First,C,Southampton,n
0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y
</code>
</pre>
- 각 행은 하나의 데이터를 의미하고, 각 데이터 속에는 다양한 **속성**들이 존재한다.
- 가장 첫 번째 열은 생존유무를 나타내는 것으로, 타이타닉 생존자 예측 문제에서 이는 각 데이터의 정답, 즉 **라벨**이 된다.
<pre>
<code class="python">
LABEL_COLUMN = 'survived'
LABELS = [0, 1]
</code>
</pre>
- 라벨 값을 보면, 0과 1로 구분된다. 이렇게 둘 중 하나로 분류하는 문제를 **이진 분류**(binary classification)이라고 한다(이 문제에서 0은 사망, 1은 생존을 의미하는 듯 하다.)
- 그 외의 다른 열들(성별, 나이, 운임료 등)은 머신러닝에서 활용할 수 있는 **속성 값**들이 되고, 이 속성(전체 혹은 일부)들을 묶어서 **속성 공간**에 위치시키거나 **속성 벡터**로 만들 수 있다.

- 앞서 가져온(다운받은) 데이터를 파이썬에서 처리하기 위해 읽어오는 함수를 사용한다(예전의 예제들은 pandas라는 패키지를 이용해서 csv파일을 읽어오고 하는 과정을 했던 것 같은데, 이 튜토리얼에서는 tensorflow의 make_csv_dataset이라는 함수를 사용한다.)
<pre>
<code class="python">
def get_dataset(file_path, **kwargs):
  dataset = tf.data.experimental.make_csv_dataset(
      file_path,
      batch_size=5, # Artificially small to make examples easier to show.
      label_name=LABEL_COLUMN,
      na_value="?",
      num_epochs=1,
      ignore_errors=True, 
      **kwargs)
  return dataset

raw_train_data = get_dataset(train_file_path)
raw_test_data = get_dataset(test_file_path)
</code>
</pre>
- 살펴보면, 함수에 csv파일의 위치와 배치(한번에 처리하는 단위?)크기, 라벨이름, na value 등 다양한 매개변수가 입력되는 것을 확인할 수 있다.
- 친절하게 배치 단위로 읽어보는 함수도 있다.
<pre>
<code class="python">
def show_batch(dataset):
  for batch, label in dataset.take(1):
    for key, value in batch.items():
      print("{:20s}: {}".format(key,value.numpy()))

show_batch(raw_train_data)
</code>
</pre>
- 결과
<pre>
<code>
sex                 : [b'female' b'male' b'female' b'male' b'male']
age                 : [19. 36. 25. 26. 19.]
n_siblings_spouses  : [1 0 1 0 0]
parch               : [0 0 1 0 0]
fare                : [ 7.854 26.388 30.    18.788  6.75 ]
class               : [b'Third' b'First' b'Second' b'Third' b'Third']
deck                : [b'unknown' b'E' b'unknown' b'unknown' b'unknown']
embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Cherbourg' b'Queenstown']
alone               : [b'n' b'y' b'n' b'y' b'y']
</code>
</pre>
- 또한, 원하는 열(column)만 지정해서 가져올 수 도 있다.
<pre>
<code class="python">
SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'class', 'deck', 'alone']

temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)

show_batch(temp_dataset)
</code>
</pre>
- 결과
<pre>
<code >
age                 : [28. 41. 23. 28. 65.]
n_siblings_spouses  : [0 0 0 0 0]
class               : [b'Third' b'Second' b'Second' b'First' b'First']
deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'B']
alone               : [b'y' b'n' b'y' b'y' b'n']
</code>
</pre>

# 3. Data preprocessing
- csv 파일에는 다양한 타입의 값들이 있는데, 이 값들을 모델에 입력하기 위해서는 fixed length vector로 변환해줘야 한다.
- 이러한 과정을 data preprocessing(데이터 전처리)라고 부른다.
- 전처리를 위해서는 nltk나 sklearn과 같은 다른 패키지에서 사용되는 방식을 사용해도 되지만, 이번에는 tensorflow에서 제공하는 tf.feature_column을 이용해서 입력값들을 변환해본다.
- preprocessing을 모델에 넣으면, model을 export한 이후에도 모델에 preprocessing이 포함되어 있기 때문에 좋다!

## Continuous data
- numeric data들을 묶어서 vector로 변환할 수 있다.
- numeric data 탐색
<pre>
<code>
SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']
DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]
temp_dataset = get_dataset(train_file_path, 
                           select_columns=SELECT_COLUMNS,
                           column_defaults = DEFAULTS)

show_batch(temp_dataset)
</code>
</pre>
- 결과
<pre>
<code>
age                 : [ 1. 28. 29. 21. 28.]
n_siblings_spouses  : [1. 0. 0. 2. 0.]
parch               : [2. 0. 0. 2. 0.]
fare                : [ 20.575   0.      7.896 262.375   7.75 ]
</code>
</pre>
- 하나의 데이터에서 column들을 묶어주기 위해 다음과 같이 함수를 정의할 수 있다.
<pre>
<code>
def pack(features, label):
  return tf.stack(list(features.values()), axis=-1), label
</code>
</pre>
- 이 함수를 이용해서 데이터별로 numeric data를 묶은 것과 라벨을 출력해보면 다음과 같다.
<pre>
<code>
packed_dataset = temp_dataset.map(pack)

for features, labels in packed_dataset.take(1):
  print(features.numpy())
  print()
  print(labels.numpy())
</code>
</pre>
- 결과
<pre>
<code>
[[ 54.      1.      0.     59.4  ]
 [ 28.      0.      0.      7.25 ]
 [ 19.      3.      2.    263.   ]
 [ 30.      0.      0.     56.929]
 [ 20.      0.      0.      7.05 ]]

[1 0 0 1 0]
</code>
</pre>
- tf.feature_column의 api를 이용해서 이것들을 적절하게 처리할 수 있다. 이때, 중복되거나 필요하지 않은 것들이 섞여들어가지 않도록 주의해야 한다.
- numeric features를 하나의 column으로 묶어주기 위해서 다음과 같은 class를 정의할 수 있다.
<pre>
<code>
class PackNumericFeatures(object):
  def __init__(self, names):
    self.names = names

  def __call__(self, features, labels):
    numeric_features = [features.pop(name) for name in self.names]
    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]
    numeric_features = tf.stack(numeric_features, axis=-1)
    features['numeric'] = numeric_features

    return features, labels
</code>
</pre>
- 이를 이용해서 train과 test를 데이터를 처리하고,
<pre>
<code>
NUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']

packed_train_data = raw_train_data.map(
    PackNumericFeatures(NUMERIC_FEATURES))

packed_test_data = raw_test_data.map(
    PackNumericFeatures(NUMERIC_FEATURES))
</code>
</pre>
- batch를 출력해보면 다음과 같다.
<pre>
<code>
show_batch(packed_train_data)
</code>
</pre>
- 결과
<pre>
<code>
</code>
</pre>
- example batch 선언
<pre>
<code>
example_batch, labels_batch = next(iter(packed_train_data)) 
</code>
</pre>

### Data Normalization
- Continuous data(연속형 데이터)는 정규화(normalized) 되어야 한다(딥러닝에서는?)
- 평균과 표준편차를 이용해서 정규화를 진행하기 위해, 이를 pandas를 이용해서 구해본다.
<pre>
<code>
import pandas as pd
desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()
desc
</code>
</pre>
- pandas에서 나온 결과는 다음과 같이 사용할 수 있다.
<pre>
<code>
MEAN = np.array(desc.T['mean'])
STD = np.array(desc.T['std'])
</code>
</pre>
- 평균과 표준편차를 이용한 정규화 함수를 정의해주고,
<pre>
<code>
def normalize_numeric_data(data, mean, std):
  # Center the data
  return (data-mean)/std
</code>
</pre>
- tf.feature_columns.numeric_column API와 앞서 정의한 정규화함수를 이용해서 batch 단위로 정규화를 진행한다.
- 이때, 평균과 표준편차는 functools.partial을 이용해서 정규화 함수와 함께 사용된다.
<pre>
<code>
# See what you just created.
normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)

numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])
numeric_columns = [numeric_column]
numeric_column
</code>
</pre>
- 정의한 numeric_columns를 tf.keras.layers.DenseFeatures를 이용해 layer를 구성하고, batch를 입력해서 numeric data에 대한 preprocessing 전/후를 비교해 볼 수 있다.
- 전
<pre>
<code>
example_batch['numeric']
</code>
</pre>
- 결과값
<pre>
<code>
tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[51.   ,  0.   ,  0.   , 12.525],
       [19.   ,  0.   ,  0.   ,  0.   ],
       [28.   ,  8.   ,  2.   , 69.55 ],
       [28.   ,  8.   ,  2.   , 69.55 ],
       [21.   ,  0.   ,  0.   , 10.5  ]], dtype=float32)
</code>
</pre>
- 후
<pre>
<code>
numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)
numeric_layer(example_batch).numpy()
</code>
</pre>
- 결과
<pre>
<code>
array([[ 1.708, -0.474, -0.479, -0.4  ],
       [-0.85 , -0.474, -0.479, -0.63 ],
       [-0.13 ,  6.476,  2.043,  0.644],
       [-0.13 ,  6.476,  2.043,  0.644],
       [-0.69 , -0.474, -0.479, -0.437]], dtype=float32)
</code>
</pre>